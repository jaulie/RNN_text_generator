{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "block_size = 64\n",
    "buffer_size = 10000 # maximum amount to be held in the memory buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/jauliegoe/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-a62fc98e062666ca/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Need to use the Hugging Face CLI to load this dataset\n",
    "# Also make sure to set the package fsspec==2023.9.2 to avoid errors\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Datasets already have a split\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2119719 tiny stories.\n"
     ]
    }
   ],
   "source": [
    "#text = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "column_text = train_data['text']\n",
    "print(\"There are\", len(column_text), \"tiny stories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with\n"
     ]
    }
   ],
   "source": [
    "# We need the raw text from the columns. \n",
    "raw_text = \" \".join(column_text[0:20000]) # Lets take the first 20,000 stories to train out smaller model\n",
    "print(raw_text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the Text ##\n",
    "Now we vectorize the text by encoding the strings with a numerical representation. Each character will be mapped to a value using a function. A decoder function will also map the values back to the same characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 97  unique characters.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(raw_text))\n",
    "vocab_size = len(chars)\n",
    "print(\"The size of the vocabulary is\", vocab_size, \" unique characters.\")\n",
    "\n",
    "# Create a dict which maps each unique character to an index (Encoder)\n",
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "\n",
    "# Create a dict which maps each unique index to a character (Decoder)\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '¡', '¦', '©', '«', '±', '³', '»', 'Â', 'Ã', 'â', 'œ', '˜', '“', '”', '€', '™']\n"
     ]
    }
   ],
   "source": [
    "#Let's look at all the unique characters that are in our dataset\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StringLookup Layer in Tensorflow ###\n",
    "Alternatively, this can be done with a StringLookup Layer in Tensorflow, which converts each character to a value based on a vocabulary. An inverted StringLookup Layer with the argument `invert=True` will convert a list of values back to the corresponding characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97], shape=(97,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[b'\\n' b' ' b'!' b'\"' b'$' b'&' b\"'\" b'(' b')' b'*' b'+' b',' b'-' b'.'\n",
      " b'/' b'0' b'1' b'2' b'3' b'4' b'5' b'6' b'7' b'8' b'9' b':' b';' b'?'\n",
      " b'A' b'B' b'C' b'D' b'E' b'F' b'G' b'H' b'I' b'J' b'K' b'L' b'M' b'N'\n",
      " b'O' b'P' b'Q' b'R' b'S' b'T' b'U' b'V' b'W' b'X' b'Y' b'Z' b'a' b'b'\n",
      " b'c' b'd' b'e' b'f' b'g' b'h' b'i' b'j' b'k' b'l' b'm' b'n' b'o' b'p'\n",
      " b'q' b'r' b's' b't' b'u' b'v' b'w' b'x' b'y' b'z' b'\\xc2\\xa0' b'\\xc2\\xa1'\n",
      " b'\\xc2\\xa6' b'\\xc2\\xa9' b'\\xc2\\xab' b'\\xc2\\xb1' b'\\xc2\\xb3' b'\\xc2\\xbb'\n",
      " b'\\xc3\\x82' b'\\xc3\\x83' b'\\xc3\\xa2' b'\\xc5\\x93' b'\\xcb\\x9c'\n",
      " b'\\xe2\\x80\\x9c' b'\\xe2\\x80\\x9d' b'\\xe2\\x82\\xac' b'\\xe2\\x84\\xa2'], shape=(97,), dtype=string)\n",
      "tf.Tensor([36 59 66 66 69  2 51 69 72 66 58], shape=(11,), dtype=int64)\n",
      "b'Hello World'\n"
     ]
    }
   ],
   "source": [
    "# StringLookup Layer\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=chars, mask_token=None)\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=chars, mask_token=None, invert=True)\n",
    "ids = ids_from_chars(chars)\n",
    "print(ids)\n",
    "invert = chars_from_ids(ids)\n",
    "print(invert)\n",
    "\n",
    "# Function to join the characters in the Tensor back into a String \n",
    "# (they will not be in the order of the original String but in the order they appear in the vocabulary)\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy()\n",
    "\n",
    "word = ['H', 'e', 'l','l','o',' ', 'W', 'o','r','l','d']\n",
    "word_ids = ids_from_chars(word)\n",
    "print(word_ids)\n",
    "print(text_from_ids(word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Task ##\n",
    "\n",
    "### Create an Input Pipeline ###\n",
    "\n",
    "What is the most probable next character? The input to the model we will create will be a sequence of n characters, and the model will be trained to predict the character at n+1 at each step. We will use the `tf.data.Dataset.from_tensor_slices` function to create an input pipeline from a tensor of character ids called `all_ids`. Each element of the dataset corresponds to a slice of the input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'O' b'n' b'e' b' ' b'd' b'a' b'y' b',' b' ' b'a' b' ' b'l' b'i' b't'\n",
      " b't' b'l' b'e' b' ' b'g' b'i' b'r' b'l' b' ' b'n' b'a' b'm' b'e' b'd'\n",
      " b' ' b'L' b'i' b'l' b'y' b' ' b'f' b'o' b'u' b'n' b'd' b' ' b'a' b' '\n",
      " b'n' b'e' b'e' b'd' b'l' b'e' b' ' b'i' b'n' b' ' b'h' b'e' b'r' b' '\n",
      " b'r' b'o' b'o' b'm' b'.' b' ' b'S' b'h' b'e' b' ' b'k' b'n' b'e' b'w'\n",
      " b' ' b'i' b't' b' ' b'w' b'a' b's' b' ' b'd' b'i' b'f' b'f' b'i' b'c'\n",
      " b'u' b'l' b't' b' ' b't' b'o' b' ' b'p' b'l' b'a' b'y' b' ' b'w' b'i'\n",
      " b't' b'h' b' '], shape=(101,), dtype=string)\n",
      "b'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with '\n",
      "b'it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on h'\n",
      "b'er shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew'\n",
      "b' my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogeth'\n",
      "b\"er, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them becaus\"\n"
     ]
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(raw_text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids) # Create a tf.data.Dataset object for batching\n",
    "\n",
    "seq_len = 100 \n",
    "# We take seq_len+1 size batches so that when we create (input,label) pairs later, they are of length seq_len\n",
    "sequences = ids_dataset.batch(seq_len+1,drop_remainder=True)\n",
    "\n",
    "# Prints all character derived from ids in one batch\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))\n",
    "\n",
    "# Prints all words formed by the characters in 5 batches\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset ###\n",
    "Now that we have a dataset of sequences for our pipeline, we need to create a dataset of (input, label) pairs for training. The input will be the sequence and the label will be the sequence n+1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with'\n",
      "Target: b'ne day, a little girl named Lily found a needle in her room. She knew it was difficult to play with '\n"
     ]
    }
   ],
   "source": [
    "# Creates a dataset where each sequence maps to another \n",
    "# sequence of the same size but shifted forward by 1\n",
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example))\n",
    "    print(\"Target:\", text_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data and put it into new batches before feeding it to the model\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(block_size, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)) # allows tf to prefetch the next batch of data \n",
    "                                              # while the current batch is being processed by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model ###\n",
    "Create a Model using a subclass of `tf.keras.Model`. This allows for the model to be customized for this use case. \n",
    "\n",
    "This model will have three layers:\n",
    "- `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector. Creating an `embedding_dim` dimensional embedding vector for each character.\n",
    "- `tf.keras.layers.GRU`: A Gated Recurrent Unit (GRU) Layer is a type of RNN used commonly for sequence modeling. \n",
    "- `tf.keras.layers.Dense`: This is the output layer. It will output a `vocab_size` (number of unique characters) output. For each character in the vocabulary, it will have calculated a logit, the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model subclass\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, # returns sequences of outputs for each time step in the input sequence\n",
    "                                   return_state=True) # returns the hidden state in addition to the output\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model itself\n",
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 98)\n"
     ]
    }
   ],
   "source": [
    "# Test the model by sampling a batch\n",
    "example_batch = dataset.take(1)\n",
    "input_example_batch, target_example_batch = next(iter(example_batch))\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     multiple                  25088     \n",
      "                                                                 \n",
      " gru_2 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  100450    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,063,842\n",
      "Trainable params: 4,063,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 45 48 86 88 76 92 94  7 38 88 43 24 30  6 65 55 79 50 13 27  4 80 62\n",
      " 39 55 71 80 33 13 10 63 63 65 52 61 13 60 48 90 54 76  5 11  7  7 29  8\n",
      " 16 17 23 90 22  7 58 61 34 20 76  7 63 76 56 38 37 50 77 46 49  6 69 16\n",
      " 27 50 84 96 69  7 44 55 88 33 95  5 92 23 25  1 75 46 15 12 79 94 72 82\n",
      " 88 18  7 40], shape=(100,), dtype=int64)\n",
      "Input:\n",
      " b'pened. They hope he can fix the net. They want to test it again. Tom and Lily were playing in the pa'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'2QT\\xc2\\xb1\\xc2\\xbbv\\xc5\\x93\\xe2\\x80\\x9c\\'J\\xc2\\xbbO8B&kayV-;\"zhKaqzE-*iikXg-fT\\xc3\\x83Zv$+\\'\\'A(017\\xc3\\x836\\'dgF4v\\'ivbJIVwRU&o0;V\\xc2\\xa9\\xe2\\x82\\xaco\\'Pa\\xc2\\xbbE\\xe2\\x80\\x9d$\\xc5\\x9379\\nuR/,y\\xe2\\x80\\x9cr\\xc2\\xa1\\xc2\\xbb2\\'L'\n"
     ]
    }
   ],
   "source": [
    "# Check out what next characters are predicted by the untrained model for the sample batch\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1)\n",
    "print(sampled_indices)\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model ###\n",
    "The predictions so far tell us very little since the model has yet to be trained. Given the previous RNN state and the new input from this time step, we will predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) # set to return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 98)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.5833273, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An untrained, newly initialized model should have output logits that are roughly equal to one another. If this is the case, then the exponential of the mean should be approximately the size of the vocabulary. Let's check that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.83939"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the model's training using `tf.keras.Model.compile` which configures various training-related settings. We define the optimization algorithm to be used, Adam, and the loss function, Spare Categorical Cross Entropy. The optimization algorithm is responsible for updating the model's weights based on the computed gradients. Adam (Adaptive Moment Estimation) is a popular optimization algorithm for training NNs because it has adaptive learning rates, momentum, bias correction, and is pretty efficient. The loss function (defined above) is the function whose value should be minimized (optimized). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Callback ###\n",
    "A checkpoint callback is a feature used to periodically save the model's weights and training progress during the training process. It allows the creation of backups or checkpoints of your model at specific intervals during training, so that training can later be resumed from those saved points or they can be used for evaluation or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure checkpoints are saved\n",
    "checkpoint_dir = './training_checkpoints' # directory where the checkpoints will be saved\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2770/2770 [==============================] - 6283s 2s/step - loss: 1.1254\n",
      "Epoch 2/10\n",
      "2770/2770 [==============================] - 6297s 2s/step - loss: 0.8412\n",
      "Epoch 3/10\n",
      "2770/2770 [==============================] - 6439s 2s/step - loss: 0.8030\n",
      "Epoch 4/10\n",
      "2770/2770 [==============================] - 6194s 2s/step - loss: 0.7846\n",
      "Epoch 5/10\n",
      "2770/2770 [==============================] - 6183s 2s/step - loss: 0.7748\n",
      "Epoch 6/10\n",
      "2770/2770 [==============================] - 6198s 2s/step - loss: 0.7697\n",
      "Epoch 7/10\n",
      "2770/2770 [==============================] - 6291s 2s/step - loss: 0.7677\n",
      "Epoch 8/10\n",
      "2770/2770 [==============================] - 6291s 2s/step - loss: 0.7678\n",
      "Epoch 9/10\n",
      "2770/2770 [==============================] - 6309s 2s/step - loss: 0.7694\n",
      "Epoch 10/10\n",
      "2770/2770 [==============================] - 6291s 2s/step - loss: 0.7723\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text Using the Model ##\n",
    "Now, we run the pre-trained model in a loop, keeping track of its internal state as it is executed. Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model to make a single step prediction\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the single step model\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A dog, a book. It was blue and black. Sam did not know how.\n",
      "\n",
      "\"Hey, kids, Sue, to throw his food, but it still freezes.\"\n",
      "\n",
      "Lily smiled and said, \"Yes, you can play with you. A knob with them on cloth. We can take turns to fix it.\" They are still sad, but they are sad and angry. They splash and slide and swing. They found some water on a leaf on the table. It was not cold. It called from a pond.\n",
      "\n",
      "\"What is that later, it's full of you? Can you hear her heart,\" Lily said.\n",
      "\n",
      "\"Let's buy it and play again.\"\n",
      "\n",
      "Bun fanters the lemon.\n",
      "\n",
      "Lily is happy.\n",
      "\n",
      "Sara and Ben are playing in the park with music. He takes them all around in the sand and sit still ones. One day, they want to play bans. They throw the red view down the slide.\n",
      "\n",
      "But the dunns will not touch the colors. They swing on the swings. They share the laindres, but they are good.\n",
      "\n",
      "\"Let's play a game without thing, surks,\" mom says.\n",
      "\n",
      "\"Can you open the box? Wh two wind answers and pictures or lights or that monkeys. We will pull thim.\"\n",
      "\n",
      "\"Mom, w \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.0792970657348633\n"
     ]
    }
   ],
   "source": [
    "# Run the model in a loop to continue generating the next character\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['A'])\n",
    "result = [next_char]\n",
    "\n",
    "# We do 1000 steps, generating a 1000 character text sample\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7ff41ef39450>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
